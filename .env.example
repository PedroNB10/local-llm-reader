# Model Configuration
# Embedding model - used for document vectorization and search
# Recommended: nomic-embed-text, mxbai-embed-large, all-minilm
EMBEDDING_MODEL=nomic-embed-text

# LLM model - used for generating responses
# Options: llama3.2, qwen:0.5b, mistral, codellama, etc.
LLM_MODEL=qwen:0.5b

# Database Configuration
CHROMA_PATH=chroma
DATA_PATH=data

# Chunking Configuration
CHUNK_SIZE=800
CHUNK_OVERLAP=80

# Retrieval Configuration
TOP_K_RESULTS=5
